{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2015173 , -0.6833582 ],\n",
       "       [ 0.37451947, -0.8280822 ],\n",
       "       [-0.16189468, -1.2471066 ],\n",
       "       ...,\n",
       "       [ 2.3255963 ,  4.181329  ],\n",
       "       [ 0.24707289,  4.2294044 ],\n",
       "       [ 1.7190224 ,  4.8253503 ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_separableish_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(12,8))\\nplt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\\n            c = simulated_labels, alpha = .4)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        output_error_signal = target - predictions\n",
    "        \n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            print(log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6931.471805599453\n",
      "-6931.471805599453\n",
      "-6931.471805599453\n",
      "-6931.471805599453\n",
      "-6931.471805599453\n"
     ]
    }
   ],
   "source": [
    "weights = logistic_regression(simulated_separableish_features, simulated_labels,\n",
    "                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, C = 1e15)\n",
    "clf.fit(simulated_separableish_features, simulated_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scratch: 0.5\n",
      "Accuracy from sk-learn: 0.9948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_scores = np.dot(np.hstack((np.ones((simulated_separableish_features.shape[0], 1)),\n",
    "                                 simulated_separableish_features)), weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "print('Accuracy from scratch: {0}'.format((preds == simulated_labels).sum().astype(float) / len(preds)))\n",
    "print('Accuracy from sk-learn: {0}'.format(clf.score(simulated_separableish_features, simulated_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [95.23809523809523, 95.23809523809523, 100.0, 97.61904761904762, 97.61904761904762]\n",
      "Mean Accuracy: 97.143%\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    " \n",
    "\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    " \n",
    "\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor _ in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict_classification(train, row, num_neighbors)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    "\n",
    "seed(1)\n",
    "filename = 'glass.data.csv'\n",
    "dataset = load_csv(filename)[1:]\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0, 1.51761, 13.89, 3.6, 1.36, 72.73, 0.48, 7.83, 0.0, 0.0, 1],\n",
       " [3.0, 1.51618, 13.53, 3.55, 1.54, 72.99, 0.39, 7.78, 0.0, 0.0, 1],\n",
       " [4.0, 1.51766, 13.21, 3.69, 1.29, 72.61, 0.57, 8.22, 0.0, 0.0, 1],\n",
       " [5.0, 1.51742, 13.27, 3.62, 1.24, 73.08, 0.55, 8.07, 0.0, 0.0, 1],\n",
       " [6.0, 1.51596, 12.79, 3.61, 1.62, 72.97, 0.64, 8.07, 0.0, 0.26, 1],\n",
       " [7.0, 1.51743, 13.3, 3.6, 1.14, 73.09, 0.58, 8.17, 0.0, 0.0, 1],\n",
       " [8.0, 1.51756, 13.15, 3.61, 1.05, 73.24, 0.57, 8.24, 0.0, 0.0, 1],\n",
       " [9.0, 1.51918, 14.04, 3.58, 1.37, 72.08, 0.56, 8.3, 0.0, 0.0, 1],\n",
       " [10.0, 1.51755, 13.0, 3.6, 1.36, 72.99, 0.57, 8.4, 0.0, 0.11, 1],\n",
       " [11.0, 1.51571, 12.72, 3.46, 1.56, 73.2, 0.67, 8.09, 0.0, 0.24, 1],\n",
       " [12.0, 1.51763, 12.8, 3.66, 1.27, 73.01, 0.6, 8.56, 0.0, 0.0, 1],\n",
       " [13.0, 1.51589, 12.88, 3.43, 1.4, 73.28, 0.69, 8.05, 0.0, 0.24, 1],\n",
       " [14.0, 1.51748, 12.86, 3.56, 1.27, 73.21, 0.54, 8.38, 0.0, 0.17, 1],\n",
       " [15.0, 1.51763, 12.61, 3.59, 1.31, 73.29, 0.58, 8.5, 0.0, 0.0, 1],\n",
       " [16.0, 1.51761, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1],\n",
       " [17.0, 1.51784, 12.68, 3.67, 1.16, 73.11, 0.61, 8.7, 0.0, 0.0, 1],\n",
       " [18.0, 1.52196, 14.36, 3.85, 0.89, 71.36, 0.15, 9.15, 0.0, 0.0, 1],\n",
       " [19.0, 1.51911, 13.9, 3.73, 1.18, 72.12, 0.06, 8.89, 0.0, 0.0, 1],\n",
       " [20.0, 1.51735, 13.02, 3.54, 1.69, 72.73, 0.54, 8.44, 0.0, 0.07, 1],\n",
       " [21.0, 1.5175, 12.82, 3.55, 1.49, 72.75, 0.54, 8.52, 0.0, 0.19, 1],\n",
       " [22.0, 1.51966, 14.77, 3.75, 0.29, 72.02, 0.03, 9.0, 0.0, 0.0, 1],\n",
       " [23.0, 1.51736, 12.78, 3.62, 1.29, 72.79, 0.59, 8.7, 0.0, 0.0, 1],\n",
       " [24.0, 1.51751, 12.81, 3.57, 1.35, 73.02, 0.62, 8.59, 0.0, 0.0, 1],\n",
       " [25.0, 1.5172, 13.38, 3.5, 1.15, 72.85, 0.5, 8.43, 0.0, 0.0, 1],\n",
       " [26.0, 1.51764, 12.98, 3.54, 1.21, 73.0, 0.65, 8.53, 0.0, 0.0, 1],\n",
       " [27.0, 1.51793, 13.21, 3.48, 1.41, 72.64, 0.59, 8.43, 0.0, 0.0, 1],\n",
       " [28.0, 1.51721, 12.87, 3.48, 1.33, 73.04, 0.56, 8.43, 0.0, 0.0, 1],\n",
       " [29.0, 1.51768, 12.56, 3.52, 1.43, 73.15, 0.57, 8.54, 0.0, 0.0, 1],\n",
       " [30.0, 1.51784, 13.08, 3.49, 1.28, 72.86, 0.6, 8.49, 0.0, 0.0, 1],\n",
       " [31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1],\n",
       " [32.0, 1.51747, 12.84, 3.5, 1.14, 73.27, 0.56, 8.55, 0.0, 0.0, 1],\n",
       " [33.0, 1.51775, 12.85, 3.48, 1.23, 72.97, 0.61, 8.56, 0.09, 0.22, 1],\n",
       " [34.0, 1.51753, 12.57, 3.47, 1.38, 73.39, 0.6, 8.55, 0.0, 0.06, 1],\n",
       " [35.0, 1.51783, 12.69, 3.54, 1.34, 72.95, 0.57, 8.75, 0.0, 0.0, 1],\n",
       " [36.0, 1.51567, 13.29, 3.45, 1.21, 72.74, 0.56, 8.57, 0.0, 0.0, 1],\n",
       " [37.0, 1.51909, 13.89, 3.53, 1.32, 71.81, 0.51, 8.78, 0.11, 0.0, 1],\n",
       " [38.0, 1.51797, 12.74, 3.48, 1.35, 72.96, 0.64, 8.68, 0.0, 0.0, 1],\n",
       " [39.0, 1.52213, 14.21, 3.82, 0.47, 71.77, 0.11, 9.57, 0.0, 0.0, 1],\n",
       " [40.0, 1.52213, 14.21, 3.82, 0.47, 71.77, 0.11, 9.57, 0.0, 0.0, 1],\n",
       " [41.0, 1.51793, 12.79, 3.5, 1.12, 73.03, 0.64, 8.77, 0.0, 0.0, 1],\n",
       " [42.0, 1.51755, 12.71, 3.42, 1.2, 73.2, 0.59, 8.64, 0.0, 0.0, 1],\n",
       " [43.0, 1.51779, 13.21, 3.39, 1.33, 72.76, 0.59, 8.59, 0.0, 0.0, 1],\n",
       " [44.0, 1.5221, 13.73, 3.84, 0.72, 71.76, 0.17, 9.74, 0.0, 0.0, 1],\n",
       " [45.0, 1.51786, 12.73, 3.43, 1.19, 72.95, 0.62, 8.76, 0.0, 0.3, 1],\n",
       " [46.0, 1.519, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1],\n",
       " [47.0, 1.51869, 13.19, 3.37, 1.18, 72.72, 0.57, 8.83, 0.0, 0.16, 1],\n",
       " [48.0, 1.52667, 13.99, 3.7, 0.71, 71.57, 0.02, 9.82, 0.0, 0.1, 1],\n",
       " [49.0, 1.52223, 13.21, 3.77, 0.79, 71.99, 0.13, 10.02, 0.0, 0.0, 1],\n",
       " [50.0, 1.51898, 13.58, 3.35, 1.23, 72.08, 0.59, 8.91, 0.0, 0.0, 1],\n",
       " [51.0, 1.5232, 13.72, 3.72, 0.51, 71.75, 0.09, 10.06, 0.0, 0.16, 1],\n",
       " [52.0, 1.51926, 13.2, 3.33, 1.28, 72.36, 0.6, 9.14, 0.0, 0.11, 1],\n",
       " [53.0, 1.51808, 13.43, 2.87, 1.19, 72.84, 0.55, 9.03, 0.0, 0.0, 1],\n",
       " [54.0, 1.51837, 13.14, 2.84, 1.28, 72.85, 0.55, 9.07, 0.0, 0.0, 1],\n",
       " [55.0, 1.51778, 13.21, 2.81, 1.29, 72.98, 0.51, 9.02, 0.0, 0.09, 1],\n",
       " [56.0, 1.51769, 12.45, 2.71, 1.29, 73.7, 0.56, 9.06, 0.0, 0.24, 1],\n",
       " [57.0, 1.51215, 12.99, 3.47, 1.12, 72.98, 0.62, 8.35, 0.0, 0.31, 1],\n",
       " [58.0, 1.51824, 12.87, 3.48, 1.29, 72.95, 0.6, 8.43, 0.0, 0.0, 1],\n",
       " [59.0, 1.51754, 13.48, 3.74, 1.17, 72.99, 0.59, 8.03, 0.0, 0.0, 1],\n",
       " [60.0, 1.51754, 13.39, 3.66, 1.19, 72.79, 0.57, 8.27, 0.0, 0.11, 1],\n",
       " [61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1],\n",
       " [62.0, 1.51977, 13.81, 3.58, 1.32, 71.72, 0.12, 8.67, 0.69, 0.0, 1],\n",
       " [63.0, 1.52172, 13.51, 3.86, 0.88, 71.79, 0.23, 9.54, 0.0, 0.11, 1],\n",
       " [64.0, 1.52227, 14.17, 3.81, 0.78, 71.35, 0.0, 9.69, 0.0, 0.0, 1],\n",
       " [65.0, 1.52172, 13.48, 3.74, 0.9, 72.01, 0.18, 9.61, 0.0, 0.07, 1],\n",
       " [66.0, 1.52099, 13.69, 3.59, 1.12, 71.96, 0.09, 9.4, 0.0, 0.0, 1],\n",
       " [67.0, 1.52152, 13.05, 3.65, 0.87, 72.22, 0.19, 9.85, 0.0, 0.17, 1],\n",
       " [68.0, 1.52152, 13.05, 3.65, 0.87, 72.32, 0.19, 9.85, 0.0, 0.17, 1],\n",
       " [69.0, 1.52152, 13.12, 3.58, 0.9, 72.2, 0.23, 9.82, 0.0, 0.16, 1],\n",
       " [70.0, 1.523, 13.31, 3.58, 0.82, 71.99, 0.12, 10.17, 0.0, 0.03, 1],\n",
       " [71.0, 1.51574, 14.86, 3.67, 1.74, 71.87, 0.16, 7.36, 0.0, 0.12, 0],\n",
       " [72.0, 1.51848, 13.64, 3.87, 1.27, 71.96, 0.54, 8.32, 0.0, 0.32, 0],\n",
       " [73.0, 1.51593, 13.09, 3.59, 1.52, 73.1, 0.67, 7.83, 0.0, 0.0, 0],\n",
       " [74.0, 1.51631, 13.34, 3.57, 1.57, 72.87, 0.61, 7.89, 0.0, 0.0, 0],\n",
       " [75.0, 1.51596, 13.02, 3.56, 1.54, 73.11, 0.72, 7.9, 0.0, 0.0, 0],\n",
       " [76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 0],\n",
       " [77.0, 1.51645, 13.44, 3.61, 1.54, 72.39, 0.66, 8.03, 0.0, 0.0, 0],\n",
       " [78.0, 1.51627, 13.0, 3.58, 1.54, 72.83, 0.61, 8.04, 0.0, 0.0, 0],\n",
       " [79.0, 1.51613, 13.92, 3.52, 1.25, 72.88, 0.37, 7.94, 0.0, 0.14, 0],\n",
       " [80.0, 1.5159, 12.82, 3.52, 1.9, 72.86, 0.69, 7.97, 0.0, 0.0, 0],\n",
       " [81.0, 1.51592, 12.86, 3.52, 2.12, 72.66, 0.69, 7.97, 0.0, 0.0, 0],\n",
       " [82.0, 1.51593, 13.25, 3.45, 1.43, 73.17, 0.61, 7.86, 0.0, 0.0, 0],\n",
       " [83.0, 1.51646, 13.41, 3.55, 1.25, 72.81, 0.68, 8.1, 0.0, 0.0, 0],\n",
       " [84.0, 1.51594, 13.09, 3.52, 1.55, 72.87, 0.68, 8.05, 0.0, 0.09, 0],\n",
       " [85.0, 1.51409, 14.25, 3.09, 2.08, 72.28, 1.1, 7.08, 0.0, 0.0, 0],\n",
       " [86.0, 1.51625, 13.36, 3.58, 1.49, 72.72, 0.45, 8.21, 0.0, 0.0, 0],\n",
       " [87.0, 1.51569, 13.24, 3.49, 1.47, 73.25, 0.38, 8.03, 0.0, 0.0, 0],\n",
       " [88.0, 1.51645, 13.4, 3.49, 1.52, 72.65, 0.67, 8.08, 0.0, 0.1, 0],\n",
       " [89.0, 1.51618, 13.01, 3.5, 1.48, 72.89, 0.6, 8.12, 0.0, 0.0, 0],\n",
       " [90.0, 1.5164, 12.55, 3.48, 1.87, 73.23, 0.63, 8.08, 0.0, 0.09, 0],\n",
       " [91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 0],\n",
       " [92.0, 1.51605, 12.9, 3.44, 1.45, 73.06, 0.44, 8.27, 0.0, 0.0, 0],\n",
       " [93.0, 1.51588, 13.12, 3.41, 1.58, 73.26, 0.07, 8.39, 0.0, 0.19, 0],\n",
       " [94.0, 1.5159, 13.24, 3.34, 1.47, 73.1, 0.39, 8.22, 0.0, 0.0, 0],\n",
       " [95.0, 1.51629, 12.71, 3.33, 1.49, 73.28, 0.67, 8.24, 0.0, 0.0, 0],\n",
       " [96.0, 1.5186, 13.36, 3.43, 1.43, 72.26, 0.51, 8.6, 0.0, 0.0, 0],\n",
       " [97.0, 1.51841, 13.02, 3.62, 1.06, 72.34, 0.64, 9.13, 0.0, 0.15, 0],\n",
       " [98.0, 1.51743, 12.2, 3.25, 1.16, 73.55, 0.62, 8.9, 0.0, 0.24, 0],\n",
       " [99.0, 1.51689, 12.67, 2.88, 1.71, 73.21, 0.73, 8.54, 0.0, 0.0, 0],\n",
       " [100.0, 1.51811, 12.96, 2.96, 1.43, 72.92, 0.6, 8.79, 0.14, 0.0, 0],\n",
       " [101.0, 1.51655, 12.75, 2.85, 1.44, 73.27, 0.57, 8.79, 0.11, 0.22, 0],\n",
       " [102.0, 1.5173, 12.35, 2.72, 1.63, 72.87, 0.7, 9.23, 0.0, 0.0, 0],\n",
       " [103.0, 1.5182, 12.62, 2.76, 0.83, 73.81, 0.35, 9.42, 0.0, 0.2, 0],\n",
       " [104.0, 1.52725, 13.8, 3.15, 0.66, 70.57, 0.08, 11.64, 0.0, 0.0, 0],\n",
       " [105.0, 1.5241, 13.83, 2.9, 1.17, 71.15, 0.08, 10.79, 0.0, 0.0, 0],\n",
       " [106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 0],\n",
       " [107.0, 1.53125, 10.73, 0.0, 2.1, 69.81, 0.58, 13.3, 3.15, 0.28, 0],\n",
       " [108.0, 1.53393, 12.3, 0.0, 1.0, 70.16, 0.12, 16.19, 0.0, 0.24, 0],\n",
       " [109.0, 1.52222, 14.43, 0.0, 1.0, 72.67, 0.1, 11.52, 0.0, 0.08, 0],\n",
       " [110.0, 1.51818, 13.72, 0.0, 0.56, 74.45, 0.0, 10.99, 0.0, 0.0, 0],\n",
       " [111.0, 1.52664, 11.23, 0.0, 0.77, 73.21, 0.0, 14.68, 0.0, 0.0, 0],\n",
       " [112.0, 1.52739, 11.02, 0.0, 0.75, 73.08, 0.0, 14.96, 0.0, 0.0, 0],\n",
       " [113.0, 1.52777, 12.64, 0.0, 0.67, 72.02, 0.06, 14.4, 0.0, 0.0, 0],\n",
       " [114.0, 1.51892, 13.46, 3.83, 1.26, 72.55, 0.57, 8.21, 0.0, 0.14, 0],\n",
       " [115.0, 1.51847, 13.1, 3.97, 1.19, 72.44, 0.6, 8.43, 0.0, 0.0, 0],\n",
       " [116.0, 1.51846, 13.41, 3.89, 1.33, 72.38, 0.51, 8.28, 0.0, 0.0, 0],\n",
       " [117.0, 1.51829, 13.24, 3.9, 1.41, 72.33, 0.55, 8.31, 0.0, 0.1, 0],\n",
       " [118.0, 1.51708, 13.72, 3.68, 1.81, 72.06, 0.64, 7.88, 0.0, 0.0, 0],\n",
       " [119.0, 1.51673, 13.3, 3.64, 1.53, 72.53, 0.65, 8.03, 0.0, 0.29, 0],\n",
       " [120.0, 1.51652, 13.56, 3.57, 1.47, 72.45, 0.64, 7.96, 0.0, 0.0, 0],\n",
       " [121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 0],\n",
       " [122.0, 1.51663, 12.93, 3.54, 1.62, 72.96, 0.64, 8.03, 0.0, 0.21, 0],\n",
       " [123.0, 1.51687, 13.23, 3.54, 1.48, 72.84, 0.56, 8.1, 0.0, 0.0, 0],\n",
       " [124.0, 1.51707, 13.48, 3.48, 1.71, 72.52, 0.62, 7.99, 0.0, 0.0, 0],\n",
       " [125.0, 1.52177, 13.2, 3.68, 1.15, 72.75, 0.54, 8.52, 0.0, 0.0, 0],\n",
       " [126.0, 1.51872, 12.93, 3.66, 1.56, 72.51, 0.58, 8.55, 0.0, 0.12, 0],\n",
       " [127.0, 1.51667, 12.94, 3.61, 1.26, 72.75, 0.56, 8.6, 0.0, 0.0, 0],\n",
       " [128.0, 1.52081, 13.78, 2.28, 1.43, 71.99, 0.49, 9.85, 0.0, 0.17, 0],\n",
       " [129.0, 1.52068, 13.55, 2.09, 1.67, 72.18, 0.53, 9.57, 0.27, 0.17, 0],\n",
       " [130.0, 1.5202, 13.98, 1.35, 1.63, 71.76, 0.39, 10.56, 0.0, 0.18, 0],\n",
       " [131.0, 1.52177, 13.75, 1.01, 1.36, 72.19, 0.33, 11.14, 0.0, 0.0, 0],\n",
       " [132.0, 1.52614, 13.7, 0.0, 1.36, 71.24, 0.19, 13.44, 0.0, 0.1, 0],\n",
       " [133.0, 1.51813, 13.43, 3.98, 1.18, 72.49, 0.58, 8.15, 0.0, 0.0, 0],\n",
       " [134.0, 1.518, 13.71, 3.93, 1.54, 71.81, 0.54, 8.21, 0.0, 0.15, 0],\n",
       " [135.0, 1.51811, 13.33, 3.85, 1.25, 72.78, 0.52, 8.12, 0.0, 0.0, 0],\n",
       " [136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 0],\n",
       " [137.0, 1.51806, 13.0, 3.8, 1.08, 73.07, 0.56, 8.38, 0.0, 0.12, 0],\n",
       " [138.0, 1.51711, 12.89, 3.62, 1.57, 72.96, 0.61, 8.11, 0.0, 0.0, 0],\n",
       " [139.0, 1.51674, 12.79, 3.52, 1.54, 73.36, 0.66, 7.9, 0.0, 0.0, 0],\n",
       " [140.0, 1.51674, 12.87, 3.56, 1.64, 73.14, 0.65, 7.99, 0.0, 0.0, 0],\n",
       " [141.0, 1.5169, 13.33, 3.54, 1.61, 72.54, 0.68, 8.11, 0.0, 0.0, 0],\n",
       " [142.0, 1.51851, 13.2, 3.63, 1.07, 72.83, 0.57, 8.41, 0.09, 0.17, 0],\n",
       " [143.0, 1.51662, 12.85, 3.51, 1.44, 73.01, 0.68, 8.23, 0.06, 0.25, 0],\n",
       " [144.0, 1.51709, 13.0, 3.47, 1.79, 72.72, 0.66, 8.18, 0.0, 0.0, 0],\n",
       " [145.0, 1.5166, 12.99, 3.18, 1.23, 72.97, 0.58, 8.81, 0.0, 0.24, 0],\n",
       " [146.0, 1.51839, 12.85, 3.67, 1.24, 72.57, 0.62, 8.68, 0.0, 0.35, 0],\n",
       " [147.0, 1.51769, 13.65, 3.66, 1.11, 72.77, 0.11, 8.6, 0.0, 0.0, 3],\n",
       " [148.0, 1.5161, 13.33, 3.53, 1.34, 72.67, 0.56, 8.33, 0.0, 0.0, 3],\n",
       " [149.0, 1.5167, 13.24, 3.57, 1.38, 72.7, 0.56, 8.44, 0.0, 0.1, 3],\n",
       " [150.0, 1.51643, 12.16, 3.52, 1.35, 72.89, 0.57, 8.53, 0.0, 0.0, 3],\n",
       " [151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3],\n",
       " [152.0, 1.52127, 14.32, 3.9, 0.83, 71.5, 0.0, 9.49, 0.0, 0.0, 3],\n",
       " [153.0, 1.51779, 13.64, 3.65, 0.65, 73.0, 0.06, 8.93, 0.0, 0.0, 3],\n",
       " [154.0, 1.5161, 13.42, 3.4, 1.22, 72.69, 0.59, 8.32, 0.0, 0.0, 3],\n",
       " [155.0, 1.51694, 12.86, 3.58, 1.31, 72.61, 0.61, 8.79, 0.0, 0.0, 3],\n",
       " [156.0, 1.51646, 13.04, 3.4, 1.26, 73.01, 0.52, 8.58, 0.0, 0.0, 3],\n",
       " [157.0, 1.51655, 13.41, 3.39, 1.28, 72.64, 0.52, 8.65, 0.0, 0.0, 3],\n",
       " [158.0, 1.52121, 14.03, 3.76, 0.58, 71.79, 0.11, 9.65, 0.0, 0.0, 3],\n",
       " [159.0, 1.51776, 13.53, 3.41, 1.52, 72.04, 0.58, 8.79, 0.0, 0.0, 3],\n",
       " [160.0, 1.51796, 13.5, 3.36, 1.63, 71.94, 0.57, 8.81, 0.0, 0.09, 3],\n",
       " [161.0, 1.51832, 13.33, 3.34, 1.54, 72.14, 0.56, 8.99, 0.0, 0.0, 3],\n",
       " [162.0, 1.51934, 13.64, 3.54, 0.75, 72.65, 0.16, 8.89, 0.15, 0.24, 3],\n",
       " [163.0, 1.52211, 14.19, 3.78, 0.91, 71.36, 0.23, 9.14, 0.0, 0.37, 3],\n",
       " [164.0, 1.51514, 14.01, 2.68, 3.5, 69.89, 1.68, 5.87, 2.2, 0.0, 5],\n",
       " [165.0, 1.51915, 12.73, 1.85, 1.86, 72.69, 0.6, 10.09, 0.0, 0.0, 5],\n",
       " [166.0, 1.52171, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5],\n",
       " [167.0, 1.52151, 11.03, 1.71, 1.56, 73.44, 0.58, 11.62, 0.0, 0.0, 5],\n",
       " [168.0, 1.51969, 12.64, 0.0, 1.65, 73.75, 0.38, 11.53, 0.0, 0.0, 5],\n",
       " [169.0, 1.51666, 12.86, 0.0, 1.83, 73.88, 0.97, 10.17, 0.0, 0.0, 5],\n",
       " [170.0, 1.51994, 13.27, 0.0, 1.76, 73.03, 0.47, 11.32, 0.0, 0.0, 5],\n",
       " [171.0, 1.52369, 13.44, 0.0, 1.58, 72.22, 0.32, 12.24, 0.0, 0.0, 5],\n",
       " [172.0, 1.51316, 13.02, 0.0, 3.04, 70.48, 6.21, 6.96, 0.0, 0.0, 5],\n",
       " [173.0, 1.51321, 13.0, 0.0, 3.02, 70.7, 6.21, 6.93, 0.0, 0.0, 5],\n",
       " [174.0, 1.52043, 13.38, 0.0, 1.4, 72.25, 0.33, 12.5, 0.0, 0.0, 5],\n",
       " [175.0, 1.52058, 12.85, 1.61, 2.17, 72.18, 0.76, 9.7, 0.24, 0.51, 5],\n",
       " [176.0, 1.52119, 12.97, 0.33, 1.51, 73.39, 0.13, 11.27, 0.0, 0.28, 5],\n",
       " [177.0, 1.51905, 14.0, 2.39, 1.56, 72.37, 0.0, 9.57, 0.0, 0.0, 2],\n",
       " [178.0, 1.51937, 13.79, 2.41, 1.19, 72.76, 0.0, 9.77, 0.0, 0.0, 2],\n",
       " [179.0, 1.51829, 14.46, 2.24, 1.62, 72.38, 0.0, 9.26, 0.0, 0.0, 2],\n",
       " [180.0, 1.51852, 14.09, 2.19, 1.66, 72.67, 0.0, 9.32, 0.0, 0.0, 2],\n",
       " [181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 2],\n",
       " [182.0, 1.51888, 14.99, 0.78, 1.74, 72.5, 0.0, 9.95, 0.0, 0.0, 2],\n",
       " [183.0, 1.51916, 14.15, 0.0, 2.09, 72.74, 0.0, 10.88, 0.0, 0.0, 2],\n",
       " [184.0, 1.51969, 14.56, 0.0, 0.56, 73.48, 0.0, 11.22, 0.0, 0.0, 2],\n",
       " [185.0, 1.51115, 17.38, 0.0, 0.34, 75.41, 0.0, 6.65, 0.0, 0.0, 2],\n",
       " [186.0, 1.51131, 13.69, 3.2, 1.81, 72.81, 1.76, 5.43, 1.19, 0.0, 4],\n",
       " [187.0, 1.51838, 14.32, 3.26, 2.22, 71.25, 1.46, 5.79, 1.63, 0.0, 4],\n",
       " [188.0, 1.52315, 13.44, 3.34, 1.23, 72.38, 0.6, 8.83, 0.0, 0.0, 4],\n",
       " [189.0, 1.52247, 14.86, 2.2, 2.06, 70.26, 0.76, 9.76, 0.0, 0.0, 4],\n",
       " [190.0, 1.52365, 15.79, 1.83, 1.31, 70.43, 0.31, 8.61, 1.68, 0.0, 4],\n",
       " [191.0, 1.51613, 13.88, 1.78, 1.79, 73.1, 0.0, 8.67, 0.76, 0.0, 4],\n",
       " [192.0, 1.51602, 14.85, 0.0, 2.38, 73.28, 0.0, 8.76, 0.64, 0.09, 4],\n",
       " [193.0, 1.51623, 14.2, 0.0, 2.79, 73.46, 0.04, 9.04, 0.4, 0.09, 4],\n",
       " [194.0, 1.51719, 14.75, 0.0, 2.0, 73.02, 0.0, 8.53, 1.59, 0.08, 4],\n",
       " [195.0, 1.51683, 14.56, 0.0, 1.98, 73.29, 0.0, 8.52, 1.57, 0.07, 4],\n",
       " [196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 4],\n",
       " [197.0, 1.51556, 13.87, 0.0, 2.54, 73.23, 0.14, 9.41, 0.81, 0.01, 4],\n",
       " [198.0, 1.51727, 14.7, 0.0, 2.34, 73.28, 0.0, 8.95, 0.66, 0.0, 4],\n",
       " [199.0, 1.51531, 14.38, 0.0, 2.66, 73.1, 0.04, 9.08, 0.64, 0.0, 4],\n",
       " [200.0, 1.51609, 15.01, 0.0, 2.51, 73.05, 0.05, 8.83, 0.53, 0.0, 4],\n",
       " [201.0, 1.51508, 15.15, 0.0, 2.25, 73.5, 0.0, 8.34, 0.63, 0.0, 4],\n",
       " [202.0, 1.51653, 11.95, 0.0, 1.19, 75.18, 2.7, 8.93, 0.0, 0.0, 4],\n",
       " [203.0, 1.51514, 14.85, 0.0, 2.42, 73.72, 0.0, 8.39, 0.56, 0.0, 4],\n",
       " [204.0, 1.51658, 14.8, 0.0, 1.99, 73.11, 0.0, 8.28, 1.71, 0.0, 4],\n",
       " [205.0, 1.51617, 14.95, 0.0, 2.27, 73.3, 0.0, 8.71, 0.67, 0.0, 4],\n",
       " [206.0, 1.51732, 14.95, 0.0, 1.8, 72.99, 0.0, 8.61, 1.55, 0.0, 4],\n",
       " [207.0, 1.51645, 14.94, 0.0, 1.87, 73.11, 0.0, 8.67, 1.38, 0.0, 4],\n",
       " [208.0, 1.51831, 14.39, 0.0, 1.82, 72.86, 1.41, 6.47, 2.88, 0.0, 4],\n",
       " [209.0, 1.5164, 14.37, 0.0, 2.74, 72.85, 0.0, 9.45, 0.54, 0.0, 4],\n",
       " [210.0, 1.51623, 14.14, 0.0, 2.88, 72.61, 0.08, 9.18, 1.06, 0.0, 4],\n",
       " [211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 4],\n",
       " [212.0, 1.52065, 14.36, 0.0, 2.02, 73.42, 0.0, 8.44, 1.64, 0.0, 4],\n",
       " [213.0, 1.51651, 14.38, 0.0, 1.94, 73.61, 0.0, 8.48, 1.57, 0.0, 4],\n",
       " [214.0, 1.51711, 14.23, 0.0, 2.08, 73.36, 0.0, 8.62, 1.67, 0.0, 4]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    train = np.array([[3.77,4.19,0],\n",
    "    [4.77,1.169761413,0],\n",
    "    [-5.,2.81281357,0],\n",
    "    [3.1,2.61995032,0],\n",
    "    [3.6,2.209014212,0],\n",
    "    [1.2,-3.162953546,1],\n",
    "    [2.3,-3.339047188,1],\n",
    "    [5.6,0.476683375,1],\n",
    "    [-1.3,-3.234550982,1],\n",
    "    [2.1,-3.319983761,1]])\n",
    "    forest = build_forest(train, k=10, N_trees=100)\n",
    "    for row in train:\n",
    "        prediction = make_prediction(forest, row)\n",
    "        #print('truth = %d : prediction = %d' % (row[-1], prediction))\n",
    "    return\n",
    "\n",
    "def traverse_tree(node, row):\n",
    "    if row[node['index']] < node['split_value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return traverse_tree(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return traverse_tree(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "def make_prediction(forest, row):\n",
    "    list_of_classes = []\n",
    "    for tree_root in forest:\n",
    "        list_of_classes.append(traverse_tree(tree_root, row))\n",
    "    return max(set(list_of_classes), key=list_of_classes.count)\n",
    "\n",
    "def calc_information_gain(groups, list_of_class_ids):\n",
    "    Nall = sum([len(group) for group in groups])\n",
    "\n",
    "    IG = calc_gini([row for group in groups for row in group], list_of_class_ids)\n",
    "    for group in groups:\n",
    "        IG -= calc_gini(group, list_of_class_ids)*len(group)/Nall\n",
    "    return IG\n",
    "\n",
    "def calc_gini(group, list_of_class_ids):\n",
    "\n",
    "    Ngroup = len(group)\n",
    "    if Ngroup == 0:\n",
    "        return 0\n",
    "    dataset_class_ids = [row[-1] for row in group]\n",
    "    sum_over_classes = 0.\n",
    "    for class_id in list_of_class_ids:\n",
    "        prob = dataset_class_ids.count(class_id)/Ngroup\n",
    "        sum_over_classes += prob**2\n",
    "    return 1. - sum_over_classes\n",
    "\n",
    "def split_node(index, value, dataset):\n",
    "    left = []\n",
    "    right = []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return [left, right]\n",
    "\n",
    "def get_split(dataset, index):\n",
    "    list_of_class_ids = list(set(row[-1] for row in dataset))\n",
    "    split_value, max_IG, split_groups = 0., -1., None\n",
    "    for row in dataset:\n",
    "        groups = split_node(index, row[index], dataset)\n",
    "        IG = calc_information_gain(groups, list_of_class_ids)\n",
    "        if IG > max_IG:\n",
    "            split_value, max_IG, split_groups = row[index], IG, groups\n",
    "    return { 'index': index, 'split_value': split_value, 'groups': groups }\n",
    "\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    feature_index = int( np.random.random()*(len(train[0]) - 1) )\n",
    "    root = get_split(train, feature_index)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "def to_terminal(group):\n",
    "    list_of_classes = [row[-1] for row in group]\n",
    "    return max(set(list_of_classes), key=list_of_classes.count)\n",
    "\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        feature_index = int( np.random.random()*(len(right[0]) - 1) )\n",
    "        node['left'] = get_split(left, feature_index)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        feature_index = int( np.random.random()*(len(right[0]) - 1) )\n",
    "        node['right'] = get_split(right, feature_index)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "def build_forest(train, k, N_trees):\n",
    "    max_depth = 4\n",
    "    min_size = 2\n",
    "    forest = []\n",
    "    for i in range(0, N_trees):\n",
    "        k_indices = np.random.choice(len(train), k)\n",
    "        forest.append(build_tree(train[k_indices], max_depth, min_size))\n",
    "    return forest\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "# С использованием random forest точность составила 97.143%, что ниже чем при использовании sk-learn (0.9948). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
