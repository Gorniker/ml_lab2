{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2015173 , -0.6833582 ],\n",
       "       [ 0.37451947, -0.8280822 ],\n",
       "       [-0.16189468, -1.2471066 ],\n",
       "       ...,\n",
       "       [ 2.3255963 ,  4.181329  ],\n",
       "       [ 0.24707289,  4.2294044 ],\n",
       "       [ 1.7190224 ,  4.8253503 ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_separableish_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(12,8))\\nplt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\\n            c = simulated_labels, alpha = .4)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        output_error_signal = target - predictions\n",
    "        \n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            print(log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6931.471805599453\n",
      "-6931.471805599453\n",
      "-6931.471805599453\n",
      "-6931.471805599453\n",
      "-6931.471805599453\n"
     ]
    }
   ],
   "source": [
    "weights = logistic_regression(simulated_separableish_features, simulated_labels,\n",
    "                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000000000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, C = 1e15)\n",
    "clf.fit(simulated_separableish_features, simulated_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scratch: 0.5\n",
      "Accuracy from sk-learn: 0.9948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_scores = np.dot(np.hstack((np.ones((simulated_separableish_features.shape[0], 1)),\n",
    "                                 simulated_separableish_features)), weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "print('Accuracy from scratch: {0}'.format((preds == simulated_labels).sum().astype(float) / len(preds)))\n",
    "print('Accuracy from sk-learn: {0}'.format(clf.score(simulated_separableish_features, simulated_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Mean Accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    " \n",
    "\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    " \n",
    "\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor _ in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict_classification(train, row, num_neighbors)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    "\n",
    "seed(1)\n",
    "filename = 'Iris.csv'\n",
    "dataset = load_csv(filename)[1:]\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 5.1, 3.5, 1.4, 0.2, 2],\n",
       " [2.0, 4.9, 3.0, 1.4, 0.2, 2],\n",
       " [3.0, 4.7, 3.2, 1.3, 0.2, 2],\n",
       " [4.0, 4.6, 3.1, 1.5, 0.2, 2],\n",
       " [5.0, 5.0, 3.6, 1.4, 0.2, 2],\n",
       " [6.0, 5.4, 3.9, 1.7, 0.4, 2],\n",
       " [7.0, 4.6, 3.4, 1.4, 0.3, 2],\n",
       " [8.0, 5.0, 3.4, 1.5, 0.2, 2],\n",
       " [9.0, 4.4, 2.9, 1.4, 0.2, 2],\n",
       " [10.0, 4.9, 3.1, 1.5, 0.1, 2],\n",
       " [11.0, 5.4, 3.7, 1.5, 0.2, 2],\n",
       " [12.0, 4.8, 3.4, 1.6, 0.2, 2],\n",
       " [13.0, 4.8, 3.0, 1.4, 0.1, 2],\n",
       " [14.0, 4.3, 3.0, 1.1, 0.1, 2],\n",
       " [15.0, 5.8, 4.0, 1.2, 0.2, 2],\n",
       " [16.0, 5.7, 4.4, 1.5, 0.4, 2],\n",
       " [17.0, 5.4, 3.9, 1.3, 0.4, 2],\n",
       " [18.0, 5.1, 3.5, 1.4, 0.3, 2],\n",
       " [19.0, 5.7, 3.8, 1.7, 0.3, 2],\n",
       " [20.0, 5.1, 3.8, 1.5, 0.3, 2],\n",
       " [21.0, 5.4, 3.4, 1.7, 0.2, 2],\n",
       " [22.0, 5.1, 3.7, 1.5, 0.4, 2],\n",
       " [23.0, 4.6, 3.6, 1.0, 0.2, 2],\n",
       " [24.0, 5.1, 3.3, 1.7, 0.5, 2],\n",
       " [25.0, 4.8, 3.4, 1.9, 0.2, 2],\n",
       " [26.0, 5.0, 3.0, 1.6, 0.2, 2],\n",
       " [27.0, 5.0, 3.4, 1.6, 0.4, 2],\n",
       " [28.0, 5.2, 3.5, 1.5, 0.2, 2],\n",
       " [29.0, 5.2, 3.4, 1.4, 0.2, 2],\n",
       " [30.0, 4.7, 3.2, 1.6, 0.2, 2],\n",
       " [31.0, 4.8, 3.1, 1.6, 0.2, 2],\n",
       " [32.0, 5.4, 3.4, 1.5, 0.4, 2],\n",
       " [33.0, 5.2, 4.1, 1.5, 0.1, 2],\n",
       " [34.0, 5.5, 4.2, 1.4, 0.2, 2],\n",
       " [35.0, 4.9, 3.1, 1.5, 0.1, 2],\n",
       " [36.0, 5.0, 3.2, 1.2, 0.2, 2],\n",
       " [37.0, 5.5, 3.5, 1.3, 0.2, 2],\n",
       " [38.0, 4.9, 3.1, 1.5, 0.1, 2],\n",
       " [39.0, 4.4, 3.0, 1.3, 0.2, 2],\n",
       " [40.0, 5.1, 3.4, 1.5, 0.2, 2],\n",
       " [41.0, 5.0, 3.5, 1.3, 0.3, 2],\n",
       " [42.0, 4.5, 2.3, 1.3, 0.3, 2],\n",
       " [43.0, 4.4, 3.2, 1.3, 0.2, 2],\n",
       " [44.0, 5.0, 3.5, 1.6, 0.6, 2],\n",
       " [45.0, 5.1, 3.8, 1.9, 0.4, 2],\n",
       " [46.0, 4.8, 3.0, 1.4, 0.3, 2],\n",
       " [47.0, 5.1, 3.8, 1.6, 0.2, 2],\n",
       " [48.0, 4.6, 3.2, 1.4, 0.2, 2],\n",
       " [49.0, 5.3, 3.7, 1.5, 0.2, 2],\n",
       " [50.0, 5.0, 3.3, 1.4, 0.2, 2],\n",
       " [51.0, 7.0, 3.2, 4.7, 1.4, 0],\n",
       " [52.0, 6.4, 3.2, 4.5, 1.5, 0],\n",
       " [53.0, 6.9, 3.1, 4.9, 1.5, 0],\n",
       " [54.0, 5.5, 2.3, 4.0, 1.3, 0],\n",
       " [55.0, 6.5, 2.8, 4.6, 1.5, 0],\n",
       " [56.0, 5.7, 2.8, 4.5, 1.3, 0],\n",
       " [57.0, 6.3, 3.3, 4.7, 1.6, 0],\n",
       " [58.0, 4.9, 2.4, 3.3, 1.0, 0],\n",
       " [59.0, 6.6, 2.9, 4.6, 1.3, 0],\n",
       " [60.0, 5.2, 2.7, 3.9, 1.4, 0],\n",
       " [61.0, 5.0, 2.0, 3.5, 1.0, 0],\n",
       " [62.0, 5.9, 3.0, 4.2, 1.5, 0],\n",
       " [63.0, 6.0, 2.2, 4.0, 1.0, 0],\n",
       " [64.0, 6.1, 2.9, 4.7, 1.4, 0],\n",
       " [65.0, 5.6, 2.9, 3.6, 1.3, 0],\n",
       " [66.0, 6.7, 3.1, 4.4, 1.4, 0],\n",
       " [67.0, 5.6, 3.0, 4.5, 1.5, 0],\n",
       " [68.0, 5.8, 2.7, 4.1, 1.0, 0],\n",
       " [69.0, 6.2, 2.2, 4.5, 1.5, 0],\n",
       " [70.0, 5.6, 2.5, 3.9, 1.1, 0],\n",
       " [71.0, 5.9, 3.2, 4.8, 1.8, 0],\n",
       " [72.0, 6.1, 2.8, 4.0, 1.3, 0],\n",
       " [73.0, 6.3, 2.5, 4.9, 1.5, 0],\n",
       " [74.0, 6.1, 2.8, 4.7, 1.2, 0],\n",
       " [75.0, 6.4, 2.9, 4.3, 1.3, 0],\n",
       " [76.0, 6.6, 3.0, 4.4, 1.4, 0],\n",
       " [77.0, 6.8, 2.8, 4.8, 1.4, 0],\n",
       " [78.0, 6.7, 3.0, 5.0, 1.7, 0],\n",
       " [79.0, 6.0, 2.9, 4.5, 1.5, 0],\n",
       " [80.0, 5.7, 2.6, 3.5, 1.0, 0],\n",
       " [81.0, 5.5, 2.4, 3.8, 1.1, 0],\n",
       " [82.0, 5.5, 2.4, 3.7, 1.0, 0],\n",
       " [83.0, 5.8, 2.7, 3.9, 1.2, 0],\n",
       " [84.0, 6.0, 2.7, 5.1, 1.6, 0],\n",
       " [85.0, 5.4, 3.0, 4.5, 1.5, 0],\n",
       " [86.0, 6.0, 3.4, 4.5, 1.6, 0],\n",
       " [87.0, 6.7, 3.1, 4.7, 1.5, 0],\n",
       " [88.0, 6.3, 2.3, 4.4, 1.3, 0],\n",
       " [89.0, 5.6, 3.0, 4.1, 1.3, 0],\n",
       " [90.0, 5.5, 2.5, 4.0, 1.3, 0],\n",
       " [91.0, 5.5, 2.6, 4.4, 1.2, 0],\n",
       " [92.0, 6.1, 3.0, 4.6, 1.4, 0],\n",
       " [93.0, 5.8, 2.6, 4.0, 1.2, 0],\n",
       " [94.0, 5.0, 2.3, 3.3, 1.0, 0],\n",
       " [95.0, 5.6, 2.7, 4.2, 1.3, 0],\n",
       " [96.0, 5.7, 3.0, 4.2, 1.2, 0],\n",
       " [97.0, 5.7, 2.9, 4.2, 1.3, 0],\n",
       " [98.0, 6.2, 2.9, 4.3, 1.3, 0],\n",
       " [99.0, 5.1, 2.5, 3.0, 1.1, 0],\n",
       " [100.0, 5.7, 2.8, 4.1, 1.3, 0],\n",
       " [101.0, 6.3, 3.3, 6.0, 2.5, 1],\n",
       " [102.0, 5.8, 2.7, 5.1, 1.9, 1],\n",
       " [103.0, 7.1, 3.0, 5.9, 2.1, 1],\n",
       " [104.0, 6.3, 2.9, 5.6, 1.8, 1],\n",
       " [105.0, 6.5, 3.0, 5.8, 2.2, 1],\n",
       " [106.0, 7.6, 3.0, 6.6, 2.1, 1],\n",
       " [107.0, 4.9, 2.5, 4.5, 1.7, 1],\n",
       " [108.0, 7.3, 2.9, 6.3, 1.8, 1],\n",
       " [109.0, 6.7, 2.5, 5.8, 1.8, 1],\n",
       " [110.0, 7.2, 3.6, 6.1, 2.5, 1],\n",
       " [111.0, 6.5, 3.2, 5.1, 2.0, 1],\n",
       " [112.0, 6.4, 2.7, 5.3, 1.9, 1],\n",
       " [113.0, 6.8, 3.0, 5.5, 2.1, 1],\n",
       " [114.0, 5.7, 2.5, 5.0, 2.0, 1],\n",
       " [115.0, 5.8, 2.8, 5.1, 2.4, 1],\n",
       " [116.0, 6.4, 3.2, 5.3, 2.3, 1],\n",
       " [117.0, 6.5, 3.0, 5.5, 1.8, 1],\n",
       " [118.0, 7.7, 3.8, 6.7, 2.2, 1],\n",
       " [119.0, 7.7, 2.6, 6.9, 2.3, 1],\n",
       " [120.0, 6.0, 2.2, 5.0, 1.5, 1],\n",
       " [121.0, 6.9, 3.2, 5.7, 2.3, 1],\n",
       " [122.0, 5.6, 2.8, 4.9, 2.0, 1],\n",
       " [123.0, 7.7, 2.8, 6.7, 2.0, 1],\n",
       " [124.0, 6.3, 2.7, 4.9, 1.8, 1],\n",
       " [125.0, 6.7, 3.3, 5.7, 2.1, 1],\n",
       " [126.0, 7.2, 3.2, 6.0, 1.8, 1],\n",
       " [127.0, 6.2, 2.8, 4.8, 1.8, 1],\n",
       " [128.0, 6.1, 3.0, 4.9, 1.8, 1],\n",
       " [129.0, 6.4, 2.8, 5.6, 2.1, 1],\n",
       " [130.0, 7.2, 3.0, 5.8, 1.6, 1],\n",
       " [131.0, 7.4, 2.8, 6.1, 1.9, 1],\n",
       " [132.0, 7.9, 3.8, 6.4, 2.0, 1],\n",
       " [133.0, 6.4, 2.8, 5.6, 2.2, 1],\n",
       " [134.0, 6.3, 2.8, 5.1, 1.5, 1],\n",
       " [135.0, 6.1, 2.6, 5.6, 1.4, 1],\n",
       " [136.0, 7.7, 3.0, 6.1, 2.3, 1],\n",
       " [137.0, 6.3, 3.4, 5.6, 2.4, 1],\n",
       " [138.0, 6.4, 3.1, 5.5, 1.8, 1],\n",
       " [139.0, 6.0, 3.0, 4.8, 1.8, 1],\n",
       " [140.0, 6.9, 3.1, 5.4, 2.1, 1],\n",
       " [141.0, 6.7, 3.1, 5.6, 2.4, 1],\n",
       " [142.0, 6.9, 3.1, 5.1, 2.3, 1],\n",
       " [143.0, 5.8, 2.7, 5.1, 1.9, 1],\n",
       " [144.0, 6.8, 3.2, 5.9, 2.3, 1],\n",
       " [145.0, 6.7, 3.3, 5.7, 2.5, 1],\n",
       " [146.0, 6.7, 3.0, 5.2, 2.3, 1],\n",
       " [147.0, 6.3, 2.5, 5.0, 1.9, 1],\n",
       " [148.0, 6.5, 3.0, 5.2, 2.0, 1],\n",
       " [149.0, 6.2, 3.4, 5.4, 2.3, 1],\n",
       " [150.0, 5.9, 3.0, 5.1, 1.8, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    train = np.array([[3.77,4.19,0],\n",
    "    [4.77,1.169761413,0],\n",
    "    [-5.,2.81281357,0],\n",
    "    [3.1,2.61995032,0],\n",
    "    [3.6,2.209014212,0],\n",
    "    [1.2,-3.162953546,1],\n",
    "    [2.3,-3.339047188,1],\n",
    "    [5.6,0.476683375,1],\n",
    "    [-1.3,-3.234550982,1],\n",
    "    [2.1,-3.319983761,1]])\n",
    "    forest = build_forest(train, k=10, N_trees=100)\n",
    "    for row in train:\n",
    "        prediction = make_prediction(forest, row)\n",
    "        #print('truth = %d : prediction = %d' % (row[-1], prediction))\n",
    "    return\n",
    "\n",
    "def traverse_tree(node, row):\n",
    "    if row[node['index']] < node['split_value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return traverse_tree(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return traverse_tree(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "def make_prediction(forest, row):\n",
    "    list_of_classes = []\n",
    "    for tree_root in forest:\n",
    "        list_of_classes.append(traverse_tree(tree_root, row))\n",
    "    return max(set(list_of_classes), key=list_of_classes.count)\n",
    "\n",
    "def calc_information_gain(groups, list_of_class_ids):\n",
    "    Nall = sum([len(group) for group in groups])\n",
    "\n",
    "    IG = calc_gini([row for group in groups for row in group], list_of_class_ids)\n",
    "    for group in groups:\n",
    "        IG -= calc_gini(group, list_of_class_ids)*len(group)/Nall\n",
    "    return IG\n",
    "\n",
    "def calc_gini(group, list_of_class_ids):\n",
    "\n",
    "    Ngroup = len(group)\n",
    "    if Ngroup == 0:\n",
    "        return 0\n",
    "    dataset_class_ids = [row[-1] for row in group]\n",
    "    sum_over_classes = 0.\n",
    "    for class_id in list_of_class_ids:\n",
    "        prob = dataset_class_ids.count(class_id)/Ngroup\n",
    "        sum_over_classes += prob**2\n",
    "    return 1. - sum_over_classes\n",
    "\n",
    "def split_node(index, value, dataset):\n",
    "    left = []\n",
    "    right = []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return [left, right]\n",
    "\n",
    "def get_split(dataset, index):\n",
    "    list_of_class_ids = list(set(row[-1] for row in dataset))\n",
    "    split_value, max_IG, split_groups = 0., -1., None\n",
    "    for row in dataset:\n",
    "        groups = split_node(index, row[index], dataset)\n",
    "        IG = calc_information_gain(groups, list_of_class_ids)\n",
    "        if IG > max_IG:\n",
    "            split_value, max_IG, split_groups = row[index], IG, groups\n",
    "    return { 'index': index, 'split_value': split_value, 'groups': groups }\n",
    "\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    feature_index = int( np.random.random()*(len(train[0]) - 1) )\n",
    "    root = get_split(train, feature_index)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "def to_terminal(group):\n",
    "    list_of_classes = [row[-1] for row in group]\n",
    "    return max(set(list_of_classes), key=list_of_classes.count)\n",
    "\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        feature_index = int( np.random.random()*(len(right[0]) - 1) )\n",
    "        node['left'] = get_split(left, feature_index)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        feature_index = int( np.random.random()*(len(right[0]) - 1) )\n",
    "        node['right'] = get_split(right, feature_index)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "def build_forest(train, k, N_trees):\n",
    "    max_depth = 4\n",
    "    min_size = 2\n",
    "    forest = []\n",
    "    for i in range(0, N_trees):\n",
    "        k_indices = np.random.choice(len(train), k)\n",
    "        forest.append(build_tree(train[k_indices], max_depth, min_size))\n",
    "    return forest\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "# Точность классификации при использовании sk-learn составляет 99%, точнотсь при использовании random forest стремится к 100%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
